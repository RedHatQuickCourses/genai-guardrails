<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Implementing Guardrails :: Guardrails for AI Applications</title>
    <link rel="prev" href="unguarded.html">
    <link rel="next" href="../ibm-guardian/index.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">Guardrails for AI Applications</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="genai-guardrails" data-version="1">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">Guardrails for AI Applications</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../index.html">Home</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="index.html">Introduction</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="guardrails.html">Why Guardrails?</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="unguarded.html">Lab: LLMs with no Guardrails</a>
  </li>
  <li class="nav-item is-current-page" data-depth="2">
    <a class="nav-link" href="implementation.html">Implementing Guardrails</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../ibm-guardian/index.html">IBM Granite Guardian</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../ibm-guardian/intro.html">IBM Granite Guardian</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../ibm-guardian/lab.html">Lab: Granite Guardian</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../guardrails-ai/index.html">Guardrails AI</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../guardrails-ai/intro.html">Introduction</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../guardrails-ai/lab.html">Lab: Guardrails AI</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../meta-ai/index.html">Meta AI Guardrails</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../meta-ai/intro.html">Introduction</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../meta-ai/lab.html">Lab: Meta AI Guardrails</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../trusty-ai/index.html">Trusty AI</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../trusty-ai/intro.html">Introduction</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../trusty-ai/lab.html">Lab: Trusty AI</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Guardrails for AI Applications</span>
    <span class="version">1</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">Guardrails for AI Applications</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">1</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">Guardrails for AI Applications</a></li>
    <li><a href="index.html">Introduction</a></li>
    <li><a href="implementation.html">Implementing Guardrails</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Implementing Guardrails</h1>
<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>In the previous section, you learnt about the importance of adding Guardrails to your AI applications. Rather than manually coding the Guardrail rails and workflows, there are specialized LLMs that can analyze inputs and outputs to and from the LLM (inference) that you are using for your applications. There are multiple options when it comes to Guardrails implementation using a mixture of specialized models and SDKs targeting programming languages like Python, Node.js and Java.</p>
</div>
<div class="paragraph">
<p>At their core, AI guardrails are <strong>rules</strong>, <strong>safeguards</strong>, and <strong>policies</strong> that govern how AI models function. Their primary purpose is to provide users with reliable and ethically sound AI interactions, guide the model towards accurate and context-aware responses, proactively prevent harmful outputs, and minimize unintended consequences.</p>
</div>
<div class="paragraph">
<p>This section lists some of the most popular guardrails libraries, frameworks, and tools for AI applications, covering both input and output guardrails.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
In this course, we will mostly focus on a few free and open source (FOSS) Guardrails frameworks and libraries.
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_proprietary_guardrails_frameworks_and_tools"><a class="anchor" href="#_proprietary_guardrails_frameworks_and_tools"></a>Proprietary Guardrails Frameworks and Tools</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Most of the hyperscaler cloud providers like AWS, GCP and Azure provide their own Guardrails frameworks that are tightly integrated with their cloud services.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><a href="https://aws.amazon.com/bedrock/guardrails" target="_blank" rel="noopener">AWS Bedrock Guardrails</a>:</p>
<div class="ulist">
<ul>
<li>
<p>Manages what an AI model can and cannot say using various filters, also called policies.</p>
</li>
<li>
<p>Content filters identify and filter harmful text or images (hate speech, insults, sexual material, violence, harmful behavior, prompt attacks) with customizable strictness.</p>
</li>
<li>
<p>Word filters block specific words or phrases (e.g., swear words, competitor names).</p>
</li>
<li>
<p>Sensitive information filters look for and can block or mask private data like Social Security numbers, birthdates, and addresses, supporting custom patterns.</p>
</li>
<li>
<p>Includes a contextual grounding check to verify if the model&#8217;s answer matches its source, flagging or filtering out "hallucinations" or off-topic responses.</p>
</li>
<li>
<p>Allows setting custom messages for blocked content.</p>
</li>
</ul>
</div>
</li>
<li>
<p><a href="https://azure.microsoft.com/en-us/products/ai-services/ai-content-safety" target="_blank" rel="noopener">Azure AI Content Safety</a>:</p>
<div class="ulist">
<ul>
<li>
<p>A cloud service that helps spot and manage harmful text and images, including hate speech, violence, sexual content, and self-harm.</p>
</li>
<li>
<p>Offers Text and Image Moderation with severity scoring.</p>
</li>
<li>
<p>Provides a Content Safety Studio for testing, adjusting, and monitoring content moderation, including managing blocklists and exporting configurations.</p>
</li>
<li>
<p>Features Customization Options to create custom moderation categories and fine-tune strictness for specific fields.</p>
</li>
<li>
<p>Includes Smarter Detection tools like Prompt Shields (catching manipulative user inputs), Groundedness Checks (verifying AI answers against real sources), and Protected Content Detection (identifying copyrighted material).</p>
</li>
<li>
<p>Offers Monitoring and Analytics for tracking system performance and trends.</p>
</li>
<li>
<p>Prioritizes Security and Privacy through secure connections, encrypted data, and adherence to privacy standards.</p>
</li>
</ul>
</div>
</li>
<li>
<p><a href="https://platform.openai.com/docs/guides/moderation" target="_blank" rel="noopener">OpenAI Moderation API</a>:</p>
<div class="ulist">
<ul>
<li>
<p>A GPT-based, multi-label classifier that assesses text for violations across content safety categories such as hate, harassment, self-harm, sexual content, and violence.</p>
</li>
<li>
<p>Returns probability scores and binary labels per category, as well as an overall binary label for the content.</p>
</li>
</ul>
</div>
</li>
<li>
<p><a href="https://www.constitutional.ai/#home" target="_blank" rel="noopener">Anthropic&#8217;s Constitutional AI</a>:</p>
<div class="ulist">
<ul>
<li>
<p>A proprietary system that embeds safety at the provider level, aiming to instill high-level normative constraints during model training.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_open_source_guardrails_frameworks_and_tools"><a class="anchor" href="#_open_source_guardrails_frameworks_and_tools"></a>Open-Source Guardrails Frameworks and Tools</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The following are some popular open source frameworks and libraries for adding Guardrails:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><a href="https://trustyai.org/docs/main/main" target="_blank" rel="noopener">TrustyAI</a>:</p>
<div class="ulist">
<ul>
<li>
<p>An open-source toolkit providing Responsible AI-as-a-Service at an enterprise scale.</p>
</li>
<li>
<p>Integrated into Open Data Hub and Red Hat OpenShift AI to streamline responsible AI workflows in MLOps.</p>
</li>
<li>
<p>Offers tools for Bias Monitoring, Drift Monitoring, Explainability, Guardrails (moderating interactions between generative models and users), and LM-Eval.</p>
</li>
<li>
<p>The Guardrails Orchestrator service (underpinned by IBM&#8217;s FMS-Guardrails Orchestrator) is a core component within TrustyAI for invoking detections on LLM text generation inputs and outputs, as well as standalone detections. It allows for configurable detector services and routes for specific pipelines.</p>
</li>
</ul>
</div>
</li>
<li>
<p><a href="https://www.ibm.com/architectures/product-guides/granite-guardian" target="_blank" rel="noopener">IBM Granite Guardian</a>:</p>
<div class="ulist">
<ul>
<li>
<p>A collection of models designed to detect risks in prompts and responses, trained on instruction fine-tuned Granite language models.</p>
</li>
<li>
<p>Can help with risk detection across dimensions cataloged in the IBM AI Risk Atlas, including harm, social bias, profanity, sexual content, unethical behavior, violence, harm engagement, evasiveness, jailbreaking, and RAG safety (groundedness, context relevance, answer relevance).</p>
</li>
<li>
<p>Granite Guardian HAP model is a small (38 million parameter) language model specifically for detecting hateful language, aggressive language, and profanity.</p>
</li>
<li>
<p>Useful for risk detection in enterprise applications, including use as guardrails, RAG assessment, and function calling risk detection in agentic workflows.</p>
</li>
</ul>
</div>
</li>
<li>
<p><a href="https://www.llama.com/docs/model-cards-and-prompt-formats/llama-guard-3/" target="_blank" rel="noopener">Meta Llama Guard</a>:</p>
<div class="ulist">
<ul>
<li>
<p>An LLM-based input-output safeguard model for Human-AI conversations.</p>
</li>
<li>
<p>A fine-tuned Llama2-7b model that classifies safety risks in prompts and responses based on a predefined safety risk taxonomy.</p>
</li>
<li>
<p>Treats safety checks as instruction-following tasks and can be adapted to different categories or guidelines using zero-shot or few-shot learning.</p>
</li>
<li>
<p>Distinguishes between classifying user messages ("prompts") and agent messages ("responses").</p>
</li>
</ul>
</div>
</li>
<li>
<p><a href="https://developer.nvidia.com/nemo-guardrails" target="_blank" rel="noopener">Nvidia NeMo Guardrails</a>:</p>
<div class="ulist">
<ul>
<li>
<p>Adds a control layer between users and LLMs to improve safety and reliability.</p>
</li>
<li>
<p>Uses Colang, a programming language by Nvidia, to set rules that guide LLM responses.</p>
</li>
<li>
<p>Comes with pre-built tools for fact-checking, preventing hallucinations, and moderating harmful content.</p>
</li>
</ul>
</div>
</li>
<li>
<p><a href="https://www.guardrailsai.com/docs" target="_blank" rel="noopener">Guardrails AI</a>:</p>
<div class="ulist">
<ul>
<li>
<p>Allows users to add rules for structure, type, and quality to the outputs of LLMs using RAIL specifications (XML).</p>
</li>
<li>
<p>Works by defining RAIL specs, integrating classifier models for tasks like toxic content detection, and performing error correction by re-prompting the LLM until rules are met.</p>
</li>
<li>
<p>Primarily works for text-based outputs.</p>
</li>
</ul>
</div>
</li>
<li>
<p><a href="https://github.com/meta-llama/PurpleLlama/tree/main/LlamaFirewall" target="_blank" rel="noopener">LlamaFirewall</a>:</p>
<div class="ulist">
<ul>
<li>
<p>An open-source security-focused guardrail framework designed as a final layer of defense against security risks in AI agents.</p>
</li>
<li>
<p>Can be deployed locally on CPU and GPU for real-time processing.</p>
</li>
<li>
<p>Mitigates risks such as prompt injection, agent misalignment, and insecure code risks.</p>
</li>
<li>
<p>Integrates PromptGuard 2, Agent Alignment Checks (AlignmentCheck), and CodeShield.</p>
</li>
<li>
<p>PromptGuard 2 specifically targets universal jailbreak attempts originating from user inputs or tool outputs. It is a lightweight classifier model built using BERT-based architectures (e.g., mDeBERTa-base, DeBERTa-xsmall) designed to detect explicit jailbreaking techniques.</p>
</li>
<li>
<p>AlignmentCheck is an experimental chain-of-thought auditor that inspects agent reasoning for prompt injection and goal misalignment.</p>
</li>
<li>
<p>CodeShield is an online static analysis engine for LLM-generated code, supporting Semgrep and regex-based rules for detecting insecure coding patterns.</p>
</li>
</ul>
</div>
</li>
<li>
<p><a href="https://whylabs.ai/langkit" target="_blank" rel="noopener">WhyLabsâ€™ LangKit</a>:</p>
<div class="ulist">
<ul>
<li>
<p>An open-source tool for monitoring Large Language Models. Compare and A/B test across different LLM and prompt versions</p>
</li>
<li>
<p>Helps trace what an agent said or did. Validate and safeguard individual prompts and responses</p>
</li>
<li>
<p>Evaluate that the LLM behavior is compliant with policy. Monitor user interactions inside LLM-powered applications</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="unguarded.html">Lab: LLMs with no Guardrails</a></span>
  <span class="next"><a href="../ibm-guardian/index.html">IBM Granite Guardian</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
