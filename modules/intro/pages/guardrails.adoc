# Why do you need Guardrails
:navtitle: Why Guardrails?

image::guardrails.png[title=Guardrails, width="600", height="250"]

**Guardrails are not just a luxury, but a fundamental necessity for building responsible AI applications**. As AI, particularly generative AI and agentic AI, becomes increasingly integrated into our daily lives and business operations, understanding and mitigating its inherent risks is a critical task.

== What are Guardrails?

image::guardrails-fig.png[title=AI Guardrails]

At its core, an AI guardrail is a __set of rules, safeguards, and policies designed to guide how AI models, especially Large Language Models (LLMs), operate__. Their primary purpose is to ensure safe, ethical, and reliable outcomes. They act as a control mechanism, meticulously monitoring and regulating both the **input** that an AI model receives and the **output** it produces, thereby ensuring responses remain safe, accurate, and ethical.

This proactive approach helps to prevent a range of undesirable behaviors, including misuse, inherent biases, the spread of misinformation, and various security risks, all while upholding compliance with industry standards and legal requirements.

video::FLOXGvqdwbM[youtube,title=What are guardrails for LLMs?,width=700,height=450]

== Why Guardrails are Indispensable for AI Applications

Just as a guardrail (fence) around a winding mountain road is used to ensure the safety of vehicles, AI guardrails channel the powerful capabilities of AI models along safe and productive pathways, preventing missteps and ensuring that the incredible potential of AI is realized responsibly.

The deployment of AI applications, particularly those powered by large language models and exhibiting agentic behavior, fundamentally transforms the digital landscape. While the capabilities are immense, so too are the risks associated with bias, misuse, unintended outputs, and security vulnerabilities. Guardrails emerge as the critical infrastructure that bridges this gap, enabling the responsible and effective deployment of AI.

Guardrails help to keep interactions with AI applications safe, useful, and aligned with intended purposes and ethical standards. By acting as a constant "digital immune system", vigilantly monitoring inputs and outputs, guardrails ensure that AI applications remain trustworthy and valuable tools, rather than becoming sources of unforeseen harm or liability. They are the essential framework for navigating the complex and evolving frontier of artificial intelligence.

While large language models can process vast amounts of information and handle diverse queries, their responses can sometimes be unpredictable or even dangerous. This inherent unpredictability makes reliable guardrails critically important, especially for organizations that must prevent issues such as:

* **Harmful Advice**: AI models could inadvertently provide guidance that leads to negative consequences, particularly in the health industry.
* **Biased Comments**: Models may perpetuate or amplify societal biases present in their training data, leading to unfair or discriminatory outputs.
* **Privacy Violations**: Without proper safeguards, AI could expose sensitive personal or confidential information.
* **Illegal Activities**: An AI application could be manipulated into suggesting or promoting unlawful actions.

Guardrails are specifically designed to address these challenges. By implementing guardrails, AI applications can provide users with reliable and ethically sound interactions, produce more accurate and context-aware responses, and proactively prevent harmful outputs, ultimately aligning AI behavior with legal and ethical standards.

=== Specific Risks Necessitating Guardrails

The IBM https://www.ibm.com/docs/en/watsonx/saas?topic=ai-risk-atlas[AI risk atlas^] outlines various risks associated with agentic AI, generative AI, and machine learning models. These risks are categorized based on whether they are traditional, amplified by generative AI, specific to generative AI, amplified by agentic AI, or specific to agentic AI. Guardrails are crucial for mitigating these diverse and evolving threats.

The appendices at the end of this section provide a brief overview of the different types of risks associated with AI applications (both generative AI and agentic AI applications) without guardrails.

== How Guardrails Work

In an unguarded system, user input flows directly to the generative model, and its output returns without intervention. This direct path creates vulnerabilities for malicious inputs or unintended outputs. Conversely, a guarded system introduces distinct "Input Guardrails", "Retrieval Guardrails", and "Output Guardrails" that intercept and moderate the flow of information, ensuring safety and compliance.

image::guardrailed-app.png[title=Guardrails Placement in an LLM Application]

The figure above visually represents the strategic placement of guardrails within an LLM application, illustrating how they intercept and influence the flow of information at key stages: input, retrieval (especially for RAG systems), and output. This ensures continuous moderation and safety.

Guardrails function by embedding a **modular set of detectors along the input and output pathways of the AI model** These detectors are configured to flag text based on predefined heuristics relevant to a specific use case. This multi-layered approach provides granular control over the AI's interactions.

* **Input Rails**: These guardrails are positioned at the initial entry point, *reviewing and processing incoming user messages*. They can filter, adjust, or completely block messages based on defined rules, effectively preventing harmful or unwanted content from ever reaching the core AI model For example, a detector could identify and stop a *prompt injection attack* before it influences the model's behavior
* **Retrieval Rails**: For AI applications that leverage external information sources, such as through Retrieval-Augmented Generation (RAG), retrieval rails ensure that **only safe and relevant data is retrieved and utilized** by the model. This helps to *ground* the model's responses in vetted information and prevent the inclusion of unreliable or toxic external content.
* **Output Rails**: Serving as the final checkpoint, these guardrails scrutinize the AI model's generated response *before it is delivered to the user*. They verify that the output adheres to ethical guidelines, complies with content policies, and meets user expectations. An example would be a detector flagging and preventing the display of *unacceptable language* or *hallucinated facts* in the model's output.

This comprehensive, layered strategy ensures that every message is thoroughly checked and refined throughout the interaction, fostering a safe and trustworthy conversation experience for users.

NOTE: While many detectors rely on small LLMs specifically tuned for Guardrails, you can also implement your own custom detectors and validators in code using the features provided by your programming language and libraries, like regular expressions and fuzzy searching.

== Additional Benefits of Guardrailing

While preventing harmful content and ensuring compliance are primary drivers, guardrails offer several other significant advantages:

* **Cost Savings**: Guardrail detectors can be substantially smaller and less resource-intensive (tens of millions of parameters) than the large generative models they protect (tens of billions of parameters). This allows organizations to *short-circuit irrelevant or off-topic queries* before they consume expensive computational resources on the main generative model, leading to considerable cost efficiencies.
* **Ensuring Appropriate Language**: Guardrails can enforce the use of language that is appropriate for the specific use case and brand. For instance, they can detect and prompt the model to rephrase overly formal language if the application is intended to be friendly and conversational
* **Authoritative Responses**: By integrating model evaluation, guardrails can ensure that the AI model only engages in discussions on subjects for which it has sufficient knowledge and authority. This prevents the model from generating unreliable information by limiting its scope to proven areas of expertise.

== Appendix

.AI Risks
****

AI agents are software entities that use AI techniques and possess the agency to act in their environment based on set goals, deciding and executing actions autonomously. Since recent agents are often built on large language models, generative AI risks are also applicable to them. The following are risks amplified by, or specific to, Agentic AI:

* **Fairness**:
** Discriminatory actions 
** Introduce data bias 

* **Privacy**:
** Sharing IP/PI/confidential information with the user 
** Sharing IP/PI/confidential information with tools

* **Value Alignment**:
** Over or under-reliance on AI agents 
** Misaligned actions 

* **Robustness** :
** Attack on AI agentsâ€™ external resources 
** Unauthorized use 
** Exploit trust mismatch 
** Function calling *hallucination* 

* **Computational Inefficiency**:
** Redundant actions 

* **Governance**:
** Incomplete AI agent evaluation 
** Mitigation and maintenance 
** Lack of AI agent transparency 
** Reproducibility 
** Accountability of AI agent actions 
** AI agent compliance 

* **Societal Impact**:
** Impact on human dignity 
** AI agents' impact on human agency
** AI agents' impact on jobs 
** AI agents' impact on the environment 

* **Explainability**:
** Unexplainable and untraceable actions

The following risks are broadly applicable to generative AI models, which form the foundation for many AI applications today.

* **Training Data Risks**:
** **Accuracy**: Unrepresentative data, Data contamination
** **Fairness**: Data bias
** **Value Alignment**: Improper data curation, Improper retraining
** **Robustness**: Data poisoning
** **Privacy**: Personal information in data, Reidentification, Data privacy rights alignment
** **Transparency**: Lack of training data transparency, Uncertain data provenance
** **Data Laws**: Data acquisition restrictions, Data usage restrictions, Data transfer restrictions
** **Intellectual Property**: Confidential information in data, Data usage rights restrictions

* **Inference Risks**:
** **Accuracy**: Poor model accuracy
** **Robustness (Model Behavior Manipulation)**: Evasion attack, Extraction attack, *Jailbreaking*
** **Intellectual Property**: IP information in prompt, Confidential data in prompt
** **Robustness (Prompt Attacks)**: *Prompt injection attack*, Prompt leaking, Prompt priming, Context overload attack, Direct instructions attack, Encoded interactions attack, Indirect instructions attack, Social hacking attack, Specialized tokens attack 
**  **Privacy**: Personal information in prompt, Attribute inference attack, Membership inference attack 

* **Output Risks** :
** **Fairness**: Decision bias, Output bias 
** **Value Alignment**: Harmful output, Harmful code generation, Toxic output, Incomplete advice, Over or under-reliance 
** **Misuse**: Dangerous use, Spreading disinformation, Nonconsensual use, Spreading toxicity, Improper usage, Non-disclosure 
** **Robustness**: *Hallucination* 
** **Privacy**: Exposing personal information 
** **Intellectual Property**: Copyright infringement, Revealing confidential information 
** **Explainability**: Unexplainable output, Unreliable source attribution, Untraceable attribution, Inaccessible training data 

* **Non-Technical Risks**:
** **Governance**: Lack of data transparency, Lack of model transparency, Lack of system transparency, Incomplete usage definition, Incorrect risk testing, Unrepresentative risk testing, Lack of testing diversity 
** **Legal Compliance**: Model usage rights restrictions, Legal accountability, Generated content ownership, and IP 
** **Societal Impact**: Impact on the environment, Impact on affected communities, Human exploitation, Impact on Jobs, AI agents' Impact on human agency, Impact on cultural diversity

****

== References

* https://www.ibm.com/docs/en/watsonx/saas?topic=ai-risk-atlas[IBM AI Risk Atlas^]
* https://www.redhat.com/en/blog/security-and-safety-ai-systems[Security and safety of AI systems^]