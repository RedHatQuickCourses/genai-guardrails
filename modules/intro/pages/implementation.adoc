# Implementing Guardrails

In the previous section, you learnt about the importance of adding Guardrails to your AI applications. Rather than manually coding the Guardrail rails and workflows, there are specialized LLMs that can analyze inputs and outputs to and from the LLM (inference) that you are using for your applications. There are multiple options when it comes to Guardrails implementation using a mixture of specialized models and SDKs targeting programming languages like Python, Node.js, and Java.

At their core, AI guardrails are *rules*, *safeguards*, and *policies* that govern how AI models function. Their primary purpose is to provide users with reliable and ethically sound AI interactions, guide the model towards accurate and context-aware responses, proactively prevent harmful outputs, and minimize unintended consequences.

This section lists some of the most popular guardrails libraries, frameworks, and tools for AI applications, covering both input and output guardrails.

NOTE: In this course, we will mostly focus on a few free and open source (FOSS) Guardrails frameworks and libraries.

== Proprietary Guardrails Frameworks and Tools

Most of the hyperscaler cloud providers like AWS, GCP, and Azure provide their own Guardrails frameworks that are tightly integrated with their cloud services.

* https://aws.amazon.com/bedrock/guardrails[AWS Bedrock Guardrails^]:
** Manages what an AI model can and cannot say using various filters, also called policies.
** Content filters identify and filter harmful text or images (hate speech, insults, sexual material, violence, harmful behavior, and prompt attacks) with customizable strictness.
** Word filters block specific words or phrases (e.g., swear words, competitor names).
** Sensitive information filters look for and can block or mask private data like Social Security numbers, birthdates, and addresses, supporting custom patterns.
** Includes a contextual grounding check to verify if the model's answer matches its source, flagging or filtering out "hallucinations" or off-topic responses.
** Allows setting custom messages for blocked content.

* https://azure.microsoft.com/en-us/products/ai-services/ai-content-safety[Azure AI Content Safety^]:
** A cloud service that helps spot and manage harmful text and images, including hate speech, violence, sexual content, and self-harm.
** Offers Text and Image Moderation with severity scoring.
** Provides a Content Safety Studio for testing, adjusting, and monitoring content moderation, including managing blocklists and exporting configurations.
** Features Customization Options to create custom moderation categories and fine-tune strictness for specific fields.
** Includes Smarter Detection tools like Prompt Shields (catching manipulative user inputs), Groundedness Checks (verifying AI answers against real sources), and Protected Content Detection (identifying copyrighted material).
** Offers monitoring and Analytics for tracking system performance and trends.
** Prioritizes Security and Privacy through secure connections, encrypted data, and adherence to privacy standards.

* https://platform.openai.com/docs/guides/moderation[OpenAI Moderation API^]:
** A GPT-based, multi-label classifier that assesses text for violations across content safety categories such as hate, harassment, self-harm, sexual content, and violence.
** Returns probability scores and binary labels per category, as well as an overall binary label for the content.

* https://www.constitutional.ai/#home[Anthropic's Constitutional AI^]:
** A proprietary system that embeds safety at the provider level, aiming to instill high-level normative constraints during model training.

== Open-Source Guardrails Frameworks and Tools

The following are some popular open source frameworks and libraries for adding Guardrails:

* https://trustyai.org/docs/main/main[TrustyAI^]:
  ** An open-source toolkit providing Responsible AI-as-a-Service at an enterprise scale.
  ** Integrated into Open Data Hub and Red Hat OpenShift AI to streamline responsible AI workflows in MLOps.
  ** Offers tools for Bias Monitoring, Drift Monitoring, Explainability, Guardrails (moderating interactions between generative models and users), and LM-Eval.
  ** The Guardrails Orchestrator service (underpinned by IBM's FMS-Guardrails Orchestrator) is a core component within TrustyAI for invoking detections on LLM text generation inputs and outputs, as well as standalone detections. It allows for configurable detector services and routes for specific pipelines.

* https://www.ibm.com/architectures/product-guides/granite-guardian[IBM Granite Guardian^]:
  ** A collection of models designed to detect risks in prompts and responses, trained on instruction fine-tuned Granite language models.
  ** Can help with risk detection across dimensions cataloged in the IBM AI Risk Atlas, including harm, social bias, profanity, sexual content, unethical behavior, violence, harm engagement, evasiveness, jailbreaking, and RAG safety (groundedness, context relevance, answer relevance).
  ** Granite Guardian HAP model is a small (38 million parameter) language model specifically for detecting hateful language, aggressive language, and profanity.
  ** Useful for risk detection in enterprise applications, including use as guardrails, RAG assessment, and function calling risk detection in agentic workflows.

* https://www.llama.com/docs/model-cards-and-prompt-formats/llama-guard-3/[Meta Llama Guard^]:
  ** An LLM-based input-output safeguard model for Human-AI conversations.
  ** A fine-tuned Llama2-7b model that classifies safety risks in prompts and responses based on a predefined safety risk taxonomy.
  ** Treats safety checks as instruction-following tasks and can be adapted to different categories or guidelines using zero-shot or few-shot learning.
  ** Distinguishes between classifying user messages ("prompts") and agent messages ("responses").

* https://developer.nvidia.com/nemo-guardrails[Nvidia NeMo Guardrails^]:
  ** Adds a control layer between users and LLMs to improve safety and reliability.
  ** Uses Colang, a programming language by Nvidia, to set rules that guide LLM responses.
  ** Comes with pre-built tools for fact-checking, preventing hallucinations, and moderating harmful content.

* https://www.guardrailsai.com/docs[Guardrails AI^]:
  ** Allows users to add rules for structure, type, and quality to the outputs of LLMs using RAIL specifications (XML).
  ** Works by defining RAIL specs, integrating classifier models for tasks like toxic content detection, and performing error correction by re-prompting the LLM until rules are met.
  ** Primarily works for text-based outputs.

* https://github.com/meta-llama/PurpleLlama/tree/main/LlamaFirewall[LlamaFirewall^]:
  ** An open-source security-focused guardrail framework designed as a final layer of defense against security risks in AI agents.
  ** Can be deployed locally on CPU and GPU for real-time processing.
  ** Mitigates risks such as prompt injection, agent misalignment, and insecure code risks.
  ** Integrates PromptGuard 2, Agent Alignment Checks (AlignmentCheck), and CodeShield.
  ** PromptGuard 2 specifically targets universal jailbreak attempts originating from user inputs or tool outputs. It is a lightweight classifier model built using BERT-based architectures (e.g., mDeBERTa-base, DeBERTa-xsmall) designed to detect explicit jailbreaking techniques.
  ** AlignmentCheck is an experimental chain-of-thought auditor that inspects agent reasoning for prompt injection and goal misalignment.
  ** CodeShield is an online static analysis engine for LLM-generated code, supporting Semgrep and regex-based rules for detecting insecure coding patterns.

* https://whylabs.ai/langkit[WhyLabsâ€™ LangKit^]:
  ** An open-source tool for monitoring Large Language Models. Compare and A/B test across different LLM and prompt versions
  ** Helps trace what an agent said or did. Validate and safeguard individual prompts and responses
  ** Evaluate that the LLM behavior is compliant with policy. Monitor user interactions inside LLM-powered applications.