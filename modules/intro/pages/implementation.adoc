# Implementing Guardrails

== Key Guardrail Frameworks and Tools

The market offers a variety of solutions, both proprietary and open-source, to implement AI guardrails:

*   **TrustyAI**:: An open-source toolkit and Red Hat team dedicated to providing Responsible AI-as-a-Service at an enterprise scale. It integrates into platforms like Open Data Hub and Red Hat OpenShift AI to streamline responsible AI workflows . TrustyAI offers a guardrails framework that allows plugging in various text detection algorithms and manages the routing between these detectors and the generative model . It also provides out-of-the-box detectors, currently regex-based, with plans for more advanced detections like validating output format (e.g., valid JSON) . A significant advantage is its centralized management, meaning changes to the guardrail configuration are transparent to consuming applications, ensuring consistent adherence to new standards across all deployments . TrustyAI also includes tools for bias monitoring, data drift monitoring, and explainability, alongside its guardrails and LM-Eval capabilities for language model evaluation.
*   **Granite Guardian**:: A collection of models from IBM specifically designed to detect risks in prompts and responses. These models are trained on instruction-tuned Granite language models and excel at identifying risks categorized in the IBM AI Risk Atlas . They cover a broad spectrum of risks, including harmful content (social bias, profanity, sexual content, violence, unethical behavior), harm engagement, evasiveness, jailbreaking, groundedness and relevance for RAG systems, and function calling hallucination in agentic workflows. The Granite-Guardian-HAP-38M model, a smaller variant, is effective for detecting hateful and profane speech.
*   **Llama Guard**:: Developed by Meta, this is an LLM-based input-output safeguard model tailored for Human-AI conversations. It leverages a safety risk taxonomy to classify risks in both user prompts and AI responses. Llama Guard treats safety checks as instruction-following tasks, guided by explicit rules and categories of unsafe content. Its instruction fine-tuning enables customization of tasks and output formats, supporting zero-shot or few-shot prompting with diverse taxonomies.
*   **Nvidia NeMo**:: This framework adds a control layer between users and LLMs to enhance safety and reliability. It uses Colang, a programming language by Nvidia, to define rules for how LLMs should respond. NeMo employs a K-nearest neighbor (KNN) method to compare user prompts with stored examples, facilitating the generation of safe responses. It also includes built-in tools for fact-checking, hallucination prevention, and harmful content moderation.
*   **Guardrails AI**:: This framework allows users to impose rules on the structure, type, and quality of LLM outputs, utilizing RAIL (Reliable AI Language) specifications, typically in XML format . Its workflow involves defining RAIL specifications, integrating classification models, and employing an error correction mechanism that can re-prompt the LLM until the output meets the defined rules . Guardrails AI is primarily text-focused and functions as a Type-2 neural-symbolic system, combining symbolic rules with machine learning models for classification .
*   **LlamaFirewall**:: An open-source, security-focused guardrail framework from Meta, designed to be the final layer of defense against security risks in AI agents . It specifically addresses prompt injection, agent misalignment, and insecure code generation risks . LlamaFirewall integrates three core guardrails:
    *   *PromptGuard 2*:: A universal jailbreak detector that operates in real-time on user prompts to identify direct jailbreak attempts .
    *   *Agent Alignment Checks*:: An experimental chain-of-thought auditor that inspects the agent's reasoning process for signs of *goal hijacking* or *prompt-injection induced misalignment* .
    *   *CodeShield*:: An online static analysis engine that uses Semgrep and regex-based rules to prevent the generation of insecure or dangerous code by coding agents .
LlamaFirewall unifies these components into a policy engine that supports custom pipelines, conditional remediation strategies, and the integration of new detectors .
*   **Proprietary Solutions**:: Major cloud providers such as AWS and Azure offer their own guardrail services.
    *   *AWS Bedrock Guardrails*:: Manages AI model output through various filter types, including content filters (for hate speech, insults, sexual material, violence, harmful behavior, prompt attacks), word filters (for specific blocked words or phrases), and sensitive information filters (for PII like SSNs, birthdates, addresses) . It also features a *contextual grounding check* to detect hallucinations and off-topic responses .
    *   *Azure AI Content Safety*:: A Microsoft cloud service for detecting and managing harmful content in text and images . It identifies hate speech, sexual content, violence, and self-harm, providing severity scores . Features include *Prompt Shields* (to catch manipulative inputs), *Groundedness Checks* (to verify AI-generated answers against real sources), and *Protected Content Detection* (for copyrighted material) .