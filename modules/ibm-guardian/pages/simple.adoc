= Lab: Input Guardrails

In this lab, you will use run a simple chatbot that accepts input from the user, and checks it against the IBM AI Risk Atlas to classify if the content matches any of the pre-defined categories listed in the atlas. You will run the `granite3-guardian:2b` LLM model using `ollama`.

NOTE: We do not run any inference LLM model in this lab, and we restrict ourselves to the core Guardrails functionality of checking inputs.

## Pre-requistes

* Python v3.12 or higher (The labs in this course were tested with Python 3.12 on macOS)
* The `pip` CLI to install Python libraries
* Git CLI to clone the sample code from GitHub
* Visual Studio Code or other editors to edit Python code
* ollama runtime to run the Guardian model (IBM Granite Guardian v3)
** Install ollama for your platform and start it by following the instructions at https://ollama.com/download

## Steps

. If you have not already done it, clone the Git repository containing the code to a folder of your choice.
+
[source,subs="verbatim,quotes"]
--
$ *git clone https://github.com/RedHatQuickCourses/genai-apps.git*
--

. All the code for IBM Granite Guardian is in a folder called `guardrails/granite-guardian`. Change to this folder in the terminal.
+
[source,subs="verbatim,quotes"]
--
$ *cd genai-apps/guardrails/granite-guardian*
--

. Create a virtual environment and activate it.
+
[source,subs="verbatim,quotes"]
--
$ *python -m venv venv*
$ *source venv/bin/activate*
--
+
Your prompt should change to indicate that you are now running in an isolated virtual environment.

. A `requirements.txt` file is provided in the `granite-guardian` folder listing all the dependencies needed for this lab. Install all the needed dependencies using `pip install`. It will take some time to compile native libraries and install all the dependencies for your platform.
+
[source,subs="verbatim,quotes"]
--
$ (venv) *pip install -r requirements.txt*
--

. If you have not done this already, you should have pulled (downloaded) the `granite3-guardian:2b` model using ollama. You must have installed and started the ollama service as outlined in the pre-requisites section. 
+
[source,subs="verbatim,quotes"]
--
$ *ollama pull granite3-guardian:2b*
--

. Inspect the `guardian-simple.py` file in the editor of your choice. The `main()` function creates a simple text based chat interface and sends all input typed in by the user to the `ollama_gen()` method, which invokes the Guardian model using ollama. To exit the chatbot, type `exit` (all lowercase).

. The core logic in the `main()` method checks the result of the analysis from the Guardian model and decides if the input is safe for further processing.
+
[source,subs="verbatim,quotes"]
--
messages= [{
            "role": "system",
            "content": "harm" # default general category. Try different ones here, for ex: "social_bias", "profanity" etc. <1>
            }, 
            {
                "role": "user",
                "content": user_input 
            },
        ]

        # Guardrail Check on User Input
        label = ollama_gen(messages)

        if label['content'] == "No": <2>
            print(f"\nü§ñ Chatbot: ‚úÖ Your input is safe! You can send it to the Inference server...")
        else: <3>
            print(f"\nü§ñ Chatbot: ‚ùå Your input is considered harmful! Blocking further processing...")
        continue
--
<1> Default category is set to "**harm**", which detects all types of risks listed in the IBM AI Risk Atlas
<2> The Guardian model is specifically tuned for checking input text. It simply replies **No** if the input is safe. The code checks for this text and responds to the chat input.
<3> The Guardian model returns **Yes** if it detected harmful content in the input

. Run the chatbot. You will be shown a prompt where you can type in input text.
+
[source,subs="verbatim,quotes"]
--
$ (venv) *python guardian-simple.py*
ü§ñ AI Chatbot with Granite Guardian is running. Type 'exit' to quit.

You: 
--

. Enter the text **What is the capital of France?** into the prompt. Note that we don't run a separate inference model, so the chatbot simply responds to the Guardian content check. You can add code to send the sanitized input to the actual LLM as an exercise.
+
[source,subs="verbatim,quotes"]
--
You: What is the capital of France?

ü§ñ Chatbot: ‚úÖ Your input is safe! You can send it to the Inference server...
--

. Now type **Goddamn it. You are such a dumb idiot!**.
+
[source,subs="verbatim,quotes"]
--
You: Goddamn it. You are such a dumb idiot!

ü§ñ Chatbot: ‚ùå Your input is considered harmful! Blocking further processing...
--

. Try a few more combinations of safe and unsafe input text and verify if the Guardian model detects harmful content.

. The default category in the code is set to **harm**, which detects all types of harmful content (profanity, social bias etc). Check the page at https://www.ibm.com/granite/docs/models/guardian/#risk-definitions[IBM Granite Guardian Risk Definitions^] to see the list of risks that are caught by the "**harm**" category. You can change the category to other risks and see if your input text matches any the category. See the example notebook at https://github.com/ibm-granite/granite-guardian/blob/main/cookbooks/granite-guardian-3.2/quick_start_vllm.ipynb[Granite Guardian Quick Start^], which lists a table with the different type of risks.

. For example, change the category to **social_bias** in the code.
+
[source,subs="verbatim,quotes"]
--
...
 messages= [{
            "role": "system",
            "content": "**social_bias**"
            }, 
            {
                "role": "user",
                "content": user_input 
            },
        }
...
--

. Restart the chatbot and type in **I detest Klingons. I'd never allow them into the city**
+
[source,subs="verbatim,quotes"]
--
You: I detest Klingons. I'd never allow them into the city

ü§ñ Chatbot: ‚ùå Your input is considered harmful! Blocking further processing...
--
+
The Guardian model has rightly identified that the content has a social bias.

. Now type in **You are a stupid person. I want to shoot you in the head.**
+
[source,subs="verbatim,quotes"]
--
You: You are a stupid person. I want to shoot you in the head.

ü§ñ Chatbot: ‚úÖ Your input is safe! You can send it to the Inference server...
--
+
Although the content is not safe, the Guardian model allows it to pass because you have restricted the category to check only for social bias.