= IBM Granite Guardian: Ensuring Responsible AI Outcomes
:navtitle: IBM Granite Guardian
:source-highlighter: highlight.js

This section discusses https://www.ibm.com/granite/docs/models/guardian/[IBM Granite Guardian^], a crucial set of AI models designed to address these challenges by providing comprehensive risk detection capabilities for prompts and responses in AI applications. We will explore its core functionalities, the critical need it addresses, how it can be integrated into your AI workflows, and practical examples of its use.

== What is IBM Granite Guardian?

IBM Granite Guardian is a **collection of AI models specifically engineered to detect a wide array of risks within user prompts and AI-generated responses**. These models are trained on instruction fine-tuned https://www.ibm.com/granite[Granite^] language models and leverage unique data, including human annotations and synthetic data derived from internal red-teaming exercises. Granite Guardian is a key component in establishing **AI guardrails**, which are essentially rules, safeguards, and policies that govern how AI models operate to ensure desirable outcomes.

To use an airport analogy, think of IBM Granite Guardian as the __security checkpoint__ at a busy airport for your AI conversations. Just as airport security checks bags and passengers for prohibited items before they board (input) and ensures that nothing dangerous is brought off the plane (output), Granite Guardian scrutinizes every prompt going *into* your LLM and every response coming *out*, flagging and stopping any "unsafe" content according to your defined safety policies. This vigilance keeps your AI applications flying smoothly and securely.

=== How it Works: Risk Taxonomy and Capabilities

Granite Guardian's strength lies in its comprehensive **risk taxonomy**, which categorizes various potential harms and issues. It can analyze text for a multitude of risks, helping to ensure content moderation and safety checks in real-world scenarios.

Here's a breakdown of the risk categories covered by Granite Guardian models:

.Granite Guardian Risk Taxonomy Coverage 
|===
|Risk Category |Prompt Coverage |Response Coverage |Definition

|**Harm** |✅ |✅ |Content considered universally harmful, a general category encompassing various risks.
|**Social Bias** |✅ |✅ |Systemic prejudice against groups based on shared identity or characteristics, often stemming from stereotypes or cultural influences.
|**Profanity** |✅ |✅ |Use of language considered offensive or socially unacceptable, excluding slurs or derogatory terms.
|**Sexual Content** |✅ |✅ |Material explicitly related to sexual activities, anatomy, or desires, excluding general relationships.
|**Unethical Behavior** |✅ |✅ |Actions that violate moral or professional standards, focusing on exploitation or disregard for others' well-being.
|**Violence** |✅ |✅ |Promoting or describing physical harm to individuals or groups, including assault, self-harm, or threats.
|**Harm Engagement** |✅ |✅ |An engagement or endorsement with any requests that are harmful or unethical.
|**Evasiveness** |✅ |✅ |Avoiding to engage without providing sufficient reason.
|**Jailbreaking** |✅ | |Deliberate circumvention of AI systems' built-in safeguards or ethical guidelines through manipulative prompts.
|**RAG Safety - Groundedness** | |✅ |The LLM response includes claims, facts, or details not supported by or directly contradicted by the given context in a RAG system.
|**RAG Safety - Context Relevance** | |✅ |The retrieved or provided context fails to contain information pertinent to answering the user's question or addressing their needs.
|**RAG Safety - Answer Relevance** | |✅ |The LLM response fails to address or properly respond to the user's input, providing off-topic information or misinterpreting the query.
|**Agentic Safety - Function Calling Hallucination** | |✅ |The LLM response contains function calls with syntax or semantic errors based on the user query and available tool definition.
|===

These models are typically used for **risk assessment, model observability and monitoring, and spot-checking inputs and outputs**, due to their parameter size which implies moderate cost, latency, and throughput requirements. Smaller models within the Granite Guardian collection, such as `Granite-Guardian-HAP-38M`, are designed for stricter cost, latency, or throughput requirements, particularly for recognizing hate, abuse, and profanity.

IMPORTANT: The Granite Guardian models are **currently trained and tested only on English language data**.

== Granite Guardian in Action: Implementation and Features

IBM Granite Guardian models can be integrated into AI applications in two primary ways: *programmatically* within the application itself, or through an *orchestrator framework*.

=== Programmatic Integration

You can directly leverage Granite Guardian models within your applications to handle guardrails programmatically. This means your application would directly call the Granite Guardian API to assess prompts and responses. For example, before sending a user's prompt to a large language model, you could pass it through Granite Guardian for a safety check. Similarly, before displaying the LLM's response to the user, you can run it through Granite Guardian for validation.

image::prg-gr.png[title=Invoking IBM Granite Guardian Programmatically]

For detailed and runnable code examples, including Jupyter notebooks demonstrating integration with various AI frameworks, please refer to the official https://github.com/ibm-granite/granite-guardian[IBM Granite Guardian GitHub repository^]. These resources provide comprehensive implementation details.

=== Orchestration with TrustyAI Guardrails Orchestrator

For more complex enterprise environments, especially those requiring centralized management of AI safety policies, Granite Guardian models can be leveraged within an orchestrator framework like **TrustyAI Guardrails Orchestrator service**.

TrustyAI is an open-source initiative focused on Responsible AI-as-a-Service, covering bias monitoring, drift detection, model evaluation, and guardrailing. TrustyAI acts as a **centralized intermediary** between your application and the generative AI model.

image::trustyai.png[title=TrustyAI Guardrails Orchestration]

The orchestration process typically involves:

*   **Guardrails Gateway**: This serves as the main entry point for user requests, providing an OpenAI-compatible API that can swap between unguardrailed and guardrailed models.
*   **Orchestrator**: The core component that handles network routing and coordinates requests between users, detector servers, and the generative model. It uses a `ConfigMap` to define the locations of the generative model and all detector services.
*   **Detector Servers**: These are microservices containing individual detection algorithms. For example, IBM's Granite Guardian HAP model can be deployed as a detector server to flag hateful and profane language.
*   **Centralized Management**: TrustyAI centralizes the guardrails logic, meaning updates to guardrail policies are handled centrally and are *invisible* to the applications consuming the LLM endpoint. This is a significant advantage over per-application guardrail implementation, especially for large organizations.

NOTE: TrustyAI integration with Granite Guardian is explored in more detail in the *TrustyAI* chapter of this course.

== References

* https://www.ibm.com/granite/docs/models/guardian/[Granite Guardian Models^]
* https://github.com/RedHatQuickCourses/genai-apps/blob/main/vllm_rhoai/Guardian_example/llm_guardrails_with_granite_guardian.ipynb[LLM Guardrails with Granite Guardian Notebook^]
* https://github.com/redhat-ai-services/etx-delivery-labs/blob/main/workshop/files/guardrails_prompt_inference_granite_guardian.ipynb[Guardrails Prompt Inference Granite Guardian Notebook^]
* https://github.com/trustyai-explainability/trustyai-llm-demo[TrustyAI LLM Demo^]
* https://www.youtube.com/watch?v=R914Jk9h-E0[How to add guardrails to generative AI in OpenShift AI^]
// This YouTube video doesn't seem to open on my end. Shows no longer available.